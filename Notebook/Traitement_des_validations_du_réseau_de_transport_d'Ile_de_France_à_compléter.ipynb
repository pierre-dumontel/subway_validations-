{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traitement_des_validations_du_réseau_de_transport_d'Ile_de_France_à_compléter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAwD7C8tx6AwGi0EKZrA+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierredumontel/subway_validations-/blob/main/Notebook/Traitement_des_validations_du_re%CC%81seau_de_transport_d'Ile_de_France_a%CC%80_comple%CC%81ter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-projet Apache Spark\n",
        "\n",
        "Merci d'indiquer les noms composant le binôme :\n",
        "\n",
        "|Nom | Prénom|\n",
        "|---|---|\n",
        "| DUMONTEL | Pierre |\n",
        "| ROUET | William |\n",
        "\n",
        "L'objectif est de collecter les données de validations quotidiennes de titres de transport de la région parisienne, disponibles en Open Data sur le site de Mobilités Ile-de-France.\n",
        "\n",
        "On se concentrera sur le **réseau ferré** exclusivement pour cet exercice."
      ],
      "metadata": {
        "id": "prg7clm-eUL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation de Spark\n",
        "\n",
        "Apache Spark n'est pas disponible en standard sur Google Colab.\n",
        "Procéder à son installation, ainsi qu'à son initialisation pour réaliser le traitement à venir."
      ],
      "metadata": {
        "id": "CwRVpp7Vebnl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWV0PL5VeFNi",
        "outputId": "2f5b6927-f76f-4990-e602-c4cb65e21e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 45.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=adb57d8310472bc2c515338f6e38538a5c2d38364b67c07f69ce831f0dea4d05\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "sql_sc = pyspark.SQLContext(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQxzv15efr5",
        "outputId": "c41baf9a-c9fd-415f-a342-efa5b218e741"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bD-2bvweqAp",
        "outputId": "74b266cc-4ab3-46e0-cdf3-dd1c2ebfb54f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (pyspark.sql.SparkSession\n",
        "         .builder\n",
        "         .appName('GasPrice')\n",
        "         .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "z6WSHtDver_L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Récupération des données\n",
        "\n",
        "Sur le site https://data.iledefrance-mobilites.fr, récupérer les données de validation par jour.\n",
        "\n",
        "Exemple :\n",
        "https://data.iledefrance-mobilites.fr/explore/dataset/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-2e-sem/information/\n",
        "\n",
        "Récupérer les données de S1 2020 et S2 2021 pour disposer d'une année complète.\n",
        "\n",
        "On utilisera pour ce faire les commandes de téléchargement de fichiers depuis un site (pas de chargement manuel).\n",
        "\n",
        "__Attention__ : prévoir une vingtaine de minutes pour le téléchargement, au moins une première fois, et donc une copie sur Google Drive si Google Colab est utilisée, afin d'éviter ce temps d'attente lors de sessions de travail successives."
      ],
      "metadata": {
        "id": "xiGEB_jfevZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://data.iledefrance-mobilites.fr/explore/dataset/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-2e-sem/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26aFi9lnew5L",
        "outputId": "2fe16522-8723-4596-b8a8-583f0cd608ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 47.4M    0 47.4M    0     0   140k      0 --:--:--  0:05:45 --:--:--  203k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P76RSwiye8RH",
        "outputId": "733b8576-a98d-48a1-f020-8903573d21da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 48M\n",
            "drwx------ 5 root root 4.0K Mar 11 10:17 drive\n",
            "drwxr-xr-x 1 root root 4.0K Mar  9 14:48 sample_data\n",
            "-rw-r--r-- 1 root root  48M Mar 11 10:23 S_TWO_Twenty.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://data.iledefrance-mobilites.fr/explore/dataset/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-sem/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcQpiA1fAs2",
        "outputId": "dba01f6f-6594-4a36-c556-fa784d47e72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 6766k    0 6766k    0     0   128k      0 --:--:--  0:00:52 --:--:--  191k"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DgVgqblQlvjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKe9wRYkfBbk",
        "outputId": "ab82bf6e-d5eb-4be4-a408-8e82abf38d9e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 79M\n",
            "drwx------ 5 root root 4.0K Mar 11 10:17 drive\n",
            "drwxr-xr-x 1 root root 4.0K Mar  9 14:48 sample_data\n",
            "-rw-r--r-- 1 root root  31M Mar 11 10:35 S_ONE_twenty_one.csv\n",
            "-rw-r--r-- 1 root root  48M Mar 11 10:23 S_TWO_Twenty.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Commentaires sur les données disponibles sur ce portail\n",
        "\n",
        "Investiguer les données diponibles sur le portail.\n",
        "\n",
        "Question : peut-on constituer un historique de données s'étendant sur les trois dernières années (2019 à 2021) ?"
      ],
      "metadata": {
        "id": "oeRlTm3PfDnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A cette adresse https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations/table/?sort=types_de_validations, il y a les données de validation du S1 2019, mais rien pour le S2 2019. Rien n'est renseigné pour le S1 2020 également. **"
      ],
      "metadata": {
        "id": "JohKpjY-fL6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture des fichiers dans Spark\n",
        "\n",
        "Lire les fichiers en choisissant les bonnes options de lecture.\n",
        "Concaténer les données en une seule table."
      ],
      "metadata": {
        "id": "XTF4do7QfPyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_TWO_twenty =spark.read.csv('/content/S_TWO_Twenty.csv', sep=';', header = True)\n",
        "S_ONE_twenty_one =spark.read.csv('/content/S_ONE_twenty_one.csv', sep=';', header = True)"
      ],
      "metadata": {
        "id": "SQyj9J_2fHb1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concaténation des fichiers"
      ],
      "metadata": {
        "id": "cmYjFXGbfWHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validations_ddf = S_ONE_twenty_one.union(S_TWO_twenty)"
      ],
      "metadata": {
        "id": "Vcf-njG_fWuD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S_TWO_twenty.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02eh_TLniCnq",
        "outputId": "c4c91f93-0b1c-4737-dfea-06cac6ff8502"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "|      jour|code_stif_trns|code_stif_res|code_stif_arret|       libelle_arret|id_refa_lda|categorie_titre|nb_vald|\n",
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "|2020-08-01|           100|          110|            691|PORTE DE LA CHAPELLE|      72064|            TST|    449|\n",
            "|2020-08-01|           100|          110|            692|PORTE DE LA VILLETTE|      72430|      IMAGINE R|    624|\n",
            "|2020-08-01|           100|          110|            692|PORTE DE LA VILLETTE|      72430|         NAVIGO|   2232|\n",
            "|2020-08-01|           100|          110|            692|PORTE DE LA VILLETTE|      72430|            TST|    729|\n",
            "|2020-08-01|           100|          110|            693|  PORTE DE MONTREUIL|      71710|      IMAGINE R|    493|\n",
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation\n",
        "\n",
        "Si vous avez appelé votre dataframe `validations_ddf`, le test suivant ne doit pas générer d'erreur."
      ],
      "metadata": {
        "id": "APVc1UI-fbGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert validations_ddf.count() == 1_316_287, \"Le nombre de lignes ne correspond pas\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "XBXfnwN2fbqc",
        "outputId": "784cd373-8fc6-4916-e01b-7a2482c2532c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-92b704e62789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mvalidations_ddf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1_316_287\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Le nombre de lignes ne correspond pas\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Le nombre de lignes ne correspond pas"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le cas où ce code génère une erreur, il s'agit probablement d'un problème de récupération ou de lecture des deux fichiers."
      ],
      "metadata": {
        "id": "1yqgnWXnfdzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation des données\n",
        "\n",
        "Réaliser les transformations nécessaires pour exploiter ces données :\n",
        "- préparation des dates\n",
        "- transformation du nombre de validation\n",
        "\n",
        "### Explication pour le nombre de validations\n",
        "\n",
        "Analyser les valeurs prises par ce champ et déterminer le problème.\n",
        "Présenter votre stratégie pour remédier à ce choix de codage par Mobilités Ile-de-France."
      ],
      "metadata": {
        "id": "_ZZLI6HZfik5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validations_ddf = (validations_ddf\n",
        " .withColumn(\"nb_vald\",validations_ddf.nb_vald.cast('int'))\n",
        " .withColumn(\"jour\",validations_ddf.jour.cast('timestamp'))\n",
        ")\n",
        "\n",
        "validations_ddf.printSchema()"
      ],
      "metadata": {
        "id": "qOgxuorJfj1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validations_ddf.show(5)"
      ],
      "metadata": {
        "id": "46kVKEgifppZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choix de codage de Mobilités Ile-de-France : le nombre de validation est renseigné par catégories de titre sur une journée. C'est à dire que pour obtenir le nombre total de validations par jour sur une seule ligne, il faudrait sommer le nombre de validations par catégories de titre pour un jour. \n",
        "Ou regrouper toutes les mêmes dates ensemble"
      ],
      "metadata": {
        "id": "uzf0GIAnfuvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Détermination des principales catégories de titre\n",
        "\n",
        "Différentes catégories de titre sont utilisées sur le réseau.\n",
        "\n",
        "Déterminer les deux catégories principalement utilisées. Seules ces catégories seront utilisées dans les travaux ci-après (les utiliser comme filtre sur les validations dans la suite)."
      ],
      "metadata": {
        "id": "7JJUiL7WfyUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F \n",
        "validations_ddf.groupBy('categorie_titre').agg(F.count('nb_vald').alias('effectif')).sort('effectif', ascending = False).show(2)"
      ],
      "metadata": {
        "id": "zjoPwz9WfsIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        ")"
      ],
      "metadata": {
        "id": "FRFi8tRjf3Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation du trafic dans une station\n",
        "\n",
        "Visualiser le trafic à la gare de Lyon pour les deux catégories de titre principales.\n",
        "\n",
        "Attention à gérer le cas des gares (comme la gare de Lyon) présentes sur plusieurs lignes et dont le libellé apparaît donc sur plusieurs lignes. Investiguer ce cas avant de déterminer la bonne façon de calculer le nombre de validations pour la gare de Lyon."
      ],
      "metadata": {
        "id": "xnGSdjxRf5yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code mettant en évidence le cas des gares sur plusieurs lignes\n",
        "\n",
        "# La colonne \"code_stif_arret\" donne le code de l'arret et/ou de la station, on voit que la Gare de Lyon accueille plusieurs arrets : \n",
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        ".filter(validations_ddf.libelle_arret == \"GARE DE LYON\")\n",
        " .groupBy('code_stif_arret').agg(F.sum('nb_vald').alias('nb de vald par arret'))\n",
        " .show()\n",
        ")"
      ],
      "metadata": {
        "id": "gfnreMDUgCvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code visualisant le trafic à la gare de Lyon\n",
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .filter(validations_ddf.libelle_arret == \"GARE DE LYON\")\n",
        " .groupBy('libelle_arret','categorie_titre','code_stif_arret').agg(F.sum('nb_vald').alias('effectif'))\n",
        " .show()\n",
        ")"
      ],
      "metadata": {
        "id": "3cL47Zn-gEj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .filter(validations_ddf.libelle_arret == \"GARE DE LYON\")\n",
        " .groupBy('libelle_arret','categorie_titre').agg(F.sum('nb_vald').alias('nb_de_vald_par_arret'))\n",
        " .show()\n",
        ")"
      ],
      "metadata": {
        "id": "nMtGEOOEgJKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fluctuation du trafic hebdomadaire\n",
        "\n",
        "Calculer le trafic total et le pourcentage par jour de la semaine sur l'ensemble du réseau.\n",
        "\n",
        "Trier le résultat par ordre décroissant de validations.\n",
        "\n",
        "Note : considérer l'usage d'une fonction analytique (`Window.partitionBy()`)."
      ],
      "metadata": {
        "id": "i2KXLFaugL3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dayofweek = sunday is 1, monday is 2,…, Saturday is 7 "
      ],
      "metadata": {
        "id": "8wIQ-f2sgPnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round\n",
        "\n",
        "prt_df = (validations_ddf\n",
        "            .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        "            .withColumn('jour_semaine', F.dayofweek(F.col('jour')))\n",
        "            .withColumn('num_semaine', F.weekofyear(F.col('jour')))\n",
        "            .toPandas()\n",
        "            )\n",
        "\n",
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .withColumn('jour_semaine', F.dayofweek(F.col('jour')))\n",
        " .withColumn('num_semaine', F.weekofyear(F.col('jour')))\n",
        " .groupBy('jour_semaine').agg(F.sum('NB_VALD').alias('nb_vald_sumcum_byday')).sort('nb_vald_sumcum_byday', ascending = False)\n",
        " .withColumn('% of total', round((F.col('nb_vald_sumcum_byday')/prt_df['nb_vald'].sum()),2))\n",
        " .show()\n",
        ")"
      ],
      "metadata": {
        "id": "SJsSuqJ9gQVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .agg(F.sum(F.col('nb_vald')))\n",
        " ).show()"
      ],
      "metadata": {
        "id": "yuhWJQX9gkBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse de l'impact du reconfinement d'octobre 2020\n",
        "\n",
        "Mettre en évidence graphiquement l'impact du reconfinement.\n",
        "\n",
        "N'utiliser que les catégories de titre _IMAGINE R_ et _Navigo_."
      ],
      "metadata": {
        "id": "HaY3TeeLgn47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le reconfinement d'octobre 2020 fut acté le 29 octobre 2020. L'idée serait de visualiser le nombre de validation par jour avant et après cette date."
      ],
      "metadata": {
        "id": "YRVmVViggrXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "import plotly.graph_objects as go\n",
        "sns.set()\n",
        "\n",
        "\n",
        "prt2_df = (validations_ddf\n",
        "            .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        "            .groupBy('jour').agg(F.sum('nb_vald').alias('tot_vald'))\n",
        "            .toPandas()\n",
        "            )\n",
        "\n",
        "\n",
        "data_prt = [go.Scatter(x = prt2_df.sort_values('jour')['jour'],\n",
        "                         y = prt2_df.sort_values('jour')['tot_vald'])]\n",
        "\n",
        "\n",
        "    # Disposition (layout)\n",
        "layout = dict(\n",
        "        title='Analyse quotidienne du trafic 07/2020-05/2021',\n",
        "        xaxis=dict(\n",
        "            rangeselector=dict(\n",
        "                buttons=list([\n",
        "                    dict(count=1,\n",
        "                         label='1d',\n",
        "                         step='day',\n",
        "                         stepmode='backward'),\n",
        "                    dict(count=6,\n",
        "                         label='6d',\n",
        "                         step='day',\n",
        "                         stepmode='backward'),\n",
        "                    dict(step='all')\n",
        "                ])\n",
        "            ),\n",
        "            rangeslider=dict(\n",
        "                visible = True\n",
        "            ),\n",
        "            type='date'\n",
        "        )\n",
        ")\n",
        "\n",
        "    # Affichage (version 4 de plotly nécessaire pour Colab)\n",
        "fig = go.Figure(\n",
        "        data=data_prt,\n",
        "        layout=layout)\n",
        "    \n",
        "fig.show()"
      ],
      "metadata": {
        "id": "JAHk7aJigoeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bonus\n",
        "\n",
        "Calculer la moyenne glissante sur 7 jours par categorie de titre pour réduire les variations hebdomadaires."
      ],
      "metadata": {
        "id": "aTVrsojigyGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_prt = (validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .groupBy('JOUR').agg(F.sum('NB_VALD').alias('nb_vald_sumcum_byday'))\n",
        " .toPandas()\n",
        ")\n",
        "\n",
        "rolling_prt = rolling_prt.sort_values('JOUR', ascending=True)\n",
        "rolling_prt[\"MA\"] = rolling_prt['nb_vald_sumcum_byday'].rolling(window=7, center = False).mean()\n",
        "rolling_qui_glisse = [go.Scatter(x = rolling_prt.sort_values('JOUR')['JOUR'],\n",
        "                         y = rolling_prt.sort_values('JOUR')['MA'])]\n",
        "\n",
        "\n",
        "    # Disposition (layout)\n",
        "layout = dict(\n",
        "        title='Moyenne glissante par période de 7 jours avec rolling',\n",
        "        xaxis=dict(\n",
        "            rangeselector=dict(\n",
        "                buttons=list([\n",
        "                    dict(count=1,\n",
        "                         label='1d',\n",
        "                         step='day',\n",
        "                         stepmode='backward'),\n",
        "                    dict(count=6,\n",
        "                         label='6d',\n",
        "                         step='day',\n",
        "                         stepmode='backward'),\n",
        "                    dict(step='all')\n",
        "                ])\n",
        "            ),\n",
        "            rangeslider=dict(\n",
        "                visible = True\n",
        "            ),\n",
        "            type='date'\n",
        "        )\n",
        ")\n",
        "\n",
        "    # Affichage (version 4 de plotly nécessaire pour Colab)\n",
        "fig = go.Figure(\n",
        "        data=rolling_qui_glisse,\n",
        "        layout=layout)\n",
        "    \n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UR_QECc1g0qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modélisation avec Apache Spark\n",
        "\n",
        "On essaie de faire un modèle basique de prévision du trafic dans les 7 prochains jours, pour une station.\n",
        "\n",
        "Apache Spark MLlib n'intègre pas de modèle pour les séries chronologiques.\n",
        "\n",
        "L'approche classique est alors d'utiliser une technique de régression classique (régression linéaire bien sûr, mais aussi RandomForestRegressor par exemple).\n",
        "\n",
        "Pour une première version simple, utiliser un vecteur constituer des validations sur les 14 jours précédents (X) pour prédire les validations du jour (y). Dans cette version, on utilisera une `LinearRegression` ou un `RandomForestRegressor`, au choix.\n",
        "\n",
        "Le code doit comporter :\n",
        "- la préparation des _features_ (X)\n",
        "- la constitution d'un ensemble d'apprentissage et de test\n",
        "- l'entrainement d'un modèle\n",
        "- le mesure de la performance du modèle : RMSE\n",
        "\n",
        "Rappel : ne travailler que sur les deux catégories de titre principales."
      ],
      "metadata": {
        "id": "blcKAR6_g5qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Première version simple \n",
        "\n",
        "Hypothèses : \n",
        "0n filtre le jeu de données sur les 2 principales catégories de titres et sur une seule station (via \"LIBELLE_ARRET\"). \n",
        "On somme le nombre de validation par jour (dans le cas où il y a plusieurs arrets dans une même station)"
      ],
      "metadata": {
        "id": "2Ekn3oFDhQMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F \n",
        "first_one = (validations_ddf\n",
        " .filter((validations_ddf.categorie_titre == \"NAVIGO\") | (validations_ddf.categorie_titre == \"IMAGINE R\"))\n",
        " .filter(validations_ddf.libelle_arret == \"GARE DE LYON\")\n",
        " .groupBy('jour').agg(F.sum('nb_vald').alias('tot_vald'))\n",
        " .sort('jour', ascending = True)\n",
        " .toPandas()\n",
        ")\n",
        "\n",
        "first_one.head()"
      ],
      "metadata": {
        "id": "z_W4yGkfhAwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Préparations des features (X)\n",
        "\n",
        "\n",
        "\n",
        "*   On construit un jeu de données avec 14 colonnes représentant le nombre total de validations pour une station les 14 jours précédents la variable à prédire (dans la première colonne, la valeur de la veille de la variable dépendante).\n",
        "*   Le premier élément du vecteur à prédire est la première observation avec un historique complet de 14 jours (soit le 15 juillet 2020)."
      ],
      "metadata": {
        "id": "Jtt9XjiDhYCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "def rolling_window(a, window):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
        "    strides = a.strides + (a.strides[-1],)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "\n",
        "X = pd.DataFrame(rolling_window(first_one['tot_vald'].to_numpy(), 14), \n",
        "                  columns=['lag14','lag13','lag12','lag11','lag10',\n",
        "                           'lag9','lag8','lag7','lag6','lag5',\n",
        "                           'lag4','lag3','lag2','lag1'])\n",
        "\n",
        "X = X[['lag1','lag2','lag3','lag4','lag5',\n",
        "         'lag6','lag7','lag8','lag9','lag10',\n",
        "         'lag11','lag12','lag13','lag14']]\n",
        "X = X.drop(index=321)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "U2IrM_RYhVhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep_mask = (first_one['jour'] > \"2020-07-14\") & (first_one['jour'] <= \"2021-05-31\")\n",
        "y = first_one.loc[dep_mask]\n",
        "y = y[[\"tot_vald\"]]\n",
        "y = y.reset_index()\n",
        "y = y.drop(columns='index')\n",
        "y.head()"
      ],
      "metadata": {
        "id": "Sr3LWYdehed3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_one.shape, y.shape, X.shape"
      ],
      "metadata": {
        "id": "hU0T5ycAhhhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Constitution d'ensembles d'apprentissage et de test \n",
        "\n",
        "On conserve les deux premiers tiers du jeu de données pour entrainer le modèle, le dernier tier fait office d'échantillon test."
      ],
      "metadata": {
        "id": "hcdA8FpRhj2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = X.iloc[:224]\n",
        "Xtest = X.iloc[224:]\n",
        "ytrain = y.iloc[:224]\n",
        "ytest = y.iloc[224:]\n",
        "\n",
        "y_test = ytest.reset_index()\n",
        "y_test = y_test.drop(columns='index')\n",
        "Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape"
      ],
      "metadata": {
        "id": "OC3T5Xq8hmHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrainement du modèle \n",
        "\n",
        "On entraine le modèle via une régression linéaire et une régression par forêts aléatoires"
      ],
      "metadata": {
        "id": "7PaXNeOShoiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "lm = linear_model.fit(X=Xtrain, y=ytrain)\n",
        "y_pred = lm.predict(Xtest)\n",
        "lm.coef_, lm.intercept_"
      ],
      "metadata": {
        "id": "KIWd7YXnhpQo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}